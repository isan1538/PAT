{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6c1f7e-a7ff-49c0-95f9-cb091a5d3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from pat_model_1208 import (\n",
    "    PATConfig, LayerNorm, PhysSelfAttention, PATBlock, \n",
    "    CrossAttention, SIRENLayer, FiLMHyper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad202c7c-0eba-4533-873e-83350e500cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def to_device(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.to(device)\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    d = os.path.dirname(path)\n",
    "    if d:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-output FiLM-SIREN\n",
    "# =============================================================================\n",
    "\n",
    "class FiLMSIRENMulti(nn.Module):\n",
    "    \"\"\"FiLM-modulated SIREN with configurable output dimension.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        width: int,\n",
    "        depth: int,\n",
    "        omega0: float,\n",
    "        hyper_in_dim: int,\n",
    "        hyper_hidden: int,\n",
    "        out_dim: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.omega0 = omega0\n",
    "        self.hyper = FiLMHyper(hyper_in_dim, hyper_hidden, depth, width)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(SIRENLayer(in_dim, width, omega0=omega0, is_first=True))\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(SIRENLayer(width, width, omega0=omega0, is_first=False))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.final = nn.Linear(width, out_dim)\n",
    "        with torch.no_grad():\n",
    "            self.final.weight.uniform_(\n",
    "                -math.sqrt(6 / width) / omega0, \n",
    "                math.sqrt(6 / width) / omega0\n",
    "            )\n",
    "\n",
    "    def forward(self, x, g, cglob):\n",
    "        B, Nq, _ = x.shape\n",
    "        g_in = torch.cat([g, cglob.expand(B, Nq, -1)], dim=-1)\n",
    "        gammas, betas, omegas = self.hyper(g_in)\n",
    "\n",
    "        h = self.layers[0](x)\n",
    "        for i, layer in enumerate(self.layers[1:], start=1):\n",
    "            gamma = gammas[i]\n",
    "            beta = betas[i]\n",
    "            omega = omegas[i]\n",
    "            h = gamma * h + beta\n",
    "            h = torch.sin(omega * layer.linear(h))\n",
    "        out = self.final(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4932261d-bf2b-4ff5-99a1-33047f63465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PAT Model for Navier-Stokes\n",
    "# =============================================================================\n",
    "\n",
    "class PATModelNS(nn.Module):\n",
    "    \"\"\"\n",
    "    PAT for 2D Navier-Stokes with three formulation modes:\n",
    "    \n",
    "    1. 'direct': Predicts (u,v,p) directly, enforces continuity via loss\n",
    "    2. 'streamfunction': Predicts (psi,p), derives u,v (continuity by construction)\n",
    "    3. 'pure': Pure transformer (no physics bias)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PATConfig, out_dim: int = 3, mode: str = 'direct'):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode  # 'direct', 'streamfunction', or 'pure'\n",
    "        \n",
    "        # If streamfunction mode, output is (psi, p) instead of (u,v,p)\n",
    "        if mode == 'streamfunction':\n",
    "            out_dim = 2\n",
    "\n",
    "        self.patch_embed = nn.Linear(cfg.d_patch, cfg.n_embd, bias=cfg.bias)\n",
    "        self.pos_enc = nn.Linear(cfg.d_pos, cfg.n_embd, bias=cfg.bias)\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, cfg.n_embd) * 0.02)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PATBlock(cfg.n_embd, cfg.n_head, cfg.dropout, cfg.bias, \n",
    "                    cfg.use_gradient_checkpointing)\n",
    "            for _ in range(cfg.n_layer)\n",
    "        ])\n",
    "        self.ln_ctx = LayerNorm(cfg.n_embd, bias=cfg.bias)\n",
    "\n",
    "        query_dim = 128\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(cfg.d_pos, query_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(query_dim, query_dim),\n",
    "        )\n",
    "\n",
    "        self.cross = CrossAttention(\n",
    "            q_dim=query_dim,\n",
    "            ctx_dim=cfg.n_embd,\n",
    "            out_dim=cfg.n_embd,\n",
    "            n_head=cfg.n_head,\n",
    "            bias=cfg.bias,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "\n",
    "        self.inr = FiLMSIRENMulti(\n",
    "            in_dim=query_dim,\n",
    "            width=128,\n",
    "            depth=4,\n",
    "            omega0=30.0,\n",
    "            hyper_in_dim=cfg.n_embd + cfg.n_embd,\n",
    "            hyper_hidden=256,\n",
    "            out_dim=out_dim,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"neg_inf\", torch.tensor(float(\"-inf\")))\n",
    "        self.register_buffer(\"neg_inf_value\", torch.tensor(-1e9))\n",
    "\n",
    "    def diffusion_bias_2d(self, ctx_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Diffusion-like physics bias in 2D for Navier-Stokes.\n",
    "        Only used if cfg.alpha > 0 and mode != 'pure'\n",
    "        \"\"\"\n",
    "        if self.mode == 'pure' or self.cfg.alpha == 0.0:\n",
    "            # Return zero bias for pure transformer mode\n",
    "            B, P, _ = ctx_pos.shape\n",
    "            return torch.zeros(B, 1, P, P, device=ctx_pos.device)\n",
    "        \n",
    "        B, P, _ = ctx_pos.shape\n",
    "        x = ctx_pos[..., 0].unsqueeze(-1)\n",
    "        y = ctx_pos[..., 1].unsqueeze(-1)\n",
    "        t = ctx_pos[..., 2].unsqueeze(-1)\n",
    "\n",
    "        dx2 = (x - x.transpose(1, 2)) ** 2\n",
    "        dy2 = (y - y.transpose(1, 2)) ** 2\n",
    "        r2 = dx2 + dy2\n",
    "\n",
    "        dt = t - t.transpose(1, 2)\n",
    "        mask = dt > 0\n",
    "\n",
    "        safe_dt = torch.clamp(dt, min=1e-6)\n",
    "        safe_r2 = torch.clamp(r2, min=1e-10)\n",
    "\n",
    "        nu = torch.clamp(torch.tensor(self.cfg.nu_bar, device=ctx_pos.device), min=1e-8)\n",
    "\n",
    "        log_term = -torch.log(torch.clamp(4 * math.pi * nu * safe_dt, min=1e-10))\n",
    "        exp_term = -safe_r2 / torch.clamp(4 * nu * safe_dt, min=1e-10)\n",
    "        exp_term = torch.clamp(exp_term, min=-50, max=50)\n",
    "\n",
    "        logG = log_term + exp_term\n",
    "        logG = torch.clamp(logG, min=-50, max=50)\n",
    "\n",
    "        logG = torch.where(mask, self.cfg.alpha * logG, self.neg_inf)\n",
    "        return logG.unsqueeze(1)\n",
    "\n",
    "    def encode_context(self, ctx_feats: torch.Tensor, ctx_pos: torch.Tensor):\n",
    "        B, P, _ = ctx_feats.shape\n",
    "\n",
    "        e = self.patch_embed(ctx_feats) + self.pos_enc(ctx_pos)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        C = torch.cat([cls, e], dim=1)\n",
    "\n",
    "        gamma = self.diffusion_bias_2d(ctx_pos)\n",
    "        gamma_full = gamma.new_zeros(B, 1, P + 1, P + 1)\n",
    "        gamma_full[:, :, 1:, 1:] = gamma\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            C = blk(C, gamma_bias=gamma_full)\n",
    "\n",
    "        C = self.ln_ctx(C)\n",
    "        cglob = C[:, :1, :]\n",
    "        C_ctx = C[:, 1:, :]\n",
    "        return C_ctx, cglob\n",
    "\n",
    "    def forward(self, ctx_feats: torch.Tensor, ctx_pos: torch.Tensor, \n",
    "                xyt_q: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            - If mode='direct': (u, v, p) with shape (B, Nq, 3)\n",
    "            - If mode='streamfunction': (psi, p) with shape (B, Nq, 2)\n",
    "        \"\"\"\n",
    "        C, cglob = self.encode_context(ctx_feats, ctx_pos)\n",
    "        phi = self.ff(xyt_q)\n",
    "        g = self.cross(phi, C)\n",
    "        out = self.inr(phi, g, cglob)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6125f390-60ed-44b8-a589-74c2c42fc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Navier-Stokes Residuals\n",
    "# =============================================================================\n",
    "\n",
    "def ns_residuals_direct(uvp: torch.Tensor, xyt: torch.Tensor, nu: float):\n",
    "    \"\"\"\n",
    "    NS residuals for direct (u,v,p) formulation.\n",
    "    \n",
    "    Args:\n",
    "        uvp: (B,N,3) => u,v,p\n",
    "        xyt: (B,N,3) => x,y,t with requires_grad=True\n",
    "        nu: kinematic viscosity\n",
    "    \n",
    "    Returns:\n",
    "        r_u, r_v, r_c: momentum-x, momentum-y, continuity residuals\n",
    "    \"\"\"\n",
    "    u = uvp[..., 0:1]\n",
    "    v = uvp[..., 1:2]\n",
    "    p = uvp[..., 2:3]\n",
    "\n",
    "    if not xyt.requires_grad:\n",
    "        xyt = xyt.requires_grad_(True)\n",
    "\n",
    "    ones_u = torch.ones_like(u)\n",
    "\n",
    "    # First derivatives\n",
    "    grads_u = torch.autograd.grad(u, xyt, grad_outputs=ones_u, \n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    grads_v = torch.autograd.grad(v, xyt, grad_outputs=torch.ones_like(v), \n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    grads_p = torch.autograd.grad(p, xyt, grad_outputs=torch.ones_like(p), \n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_x, u_y, u_t = grads_u[..., 0:1], grads_u[..., 1:2], grads_u[..., 2:3]\n",
    "    v_x, v_y, v_t = grads_v[..., 0:1], grads_v[..., 1:2], grads_v[..., 2:3]\n",
    "    p_x, p_y = grads_p[..., 0:1], grads_p[..., 1:2]\n",
    "\n",
    "    # Second derivatives (Laplacian)\n",
    "    u_xx = torch.autograd.grad(u_x, xyt, grad_outputs=torch.ones_like(u_x), \n",
    "                              create_graph=True, retain_graph=True)[0][..., 0:1]\n",
    "    u_yy = torch.autograd.grad(u_y, xyt, grad_outputs=torch.ones_like(u_y), \n",
    "                              create_graph=True, retain_graph=True)[0][..., 1:2]\n",
    "    v_xx = torch.autograd.grad(v_x, xyt, grad_outputs=torch.ones_like(v_x), \n",
    "                              create_graph=True, retain_graph=True)[0][..., 0:1]\n",
    "    v_yy = torch.autograd.grad(v_y, xyt, grad_outputs=torch.ones_like(v_y), \n",
    "                              create_graph=True, retain_graph=True)[0][..., 1:2]\n",
    "\n",
    "    # Momentum equations\n",
    "    r_u = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    r_v = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    # Continuity\n",
    "    r_c = u_x + v_y\n",
    "    return r_u, r_v, r_c\n",
    "\n",
    "\n",
    "def ns_residuals_streamfunction(psi_p: torch.Tensor, xyt: torch.Tensor, nu: float):\n",
    "    \"\"\"\n",
    "    NS residuals for streamfunction formulation.\n",
    "    \n",
    "    Args:\n",
    "        psi_p: (B,N,2) => psi, p\n",
    "        xyt: (B,N,3) => x,y,t with requires_grad=True\n",
    "        nu: kinematic viscosity\n",
    "    \n",
    "    Returns:\n",
    "        r_u, r_v: momentum residuals (continuity automatically satisfied)\n",
    "    \"\"\"\n",
    "    psi = psi_p[..., 0:1]\n",
    "    p = psi_p[..., 1:2]\n",
    "\n",
    "    if not xyt.requires_grad:\n",
    "        xyt = xyt.requires_grad_(True)\n",
    "\n",
    "    # Derive velocity from streamfunction: u = ∂psi/∂y, v = -∂psi/∂x\n",
    "    grads_psi = torch.autograd.grad(psi, xyt, grad_outputs=torch.ones_like(psi),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    psi_x = grads_psi[..., 0:1]\n",
    "    psi_y = grads_psi[..., 1:2]\n",
    "    \n",
    "    u = psi_y\n",
    "    v = -psi_x\n",
    "\n",
    "    # First derivatives of u, v\n",
    "    grads_u = torch.autograd.grad(u, xyt, grad_outputs=torch.ones_like(u),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    grads_v = torch.autograd.grad(v, xyt, grad_outputs=torch.ones_like(v),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    grads_p = torch.autograd.grad(p, xyt, grad_outputs=torch.ones_like(p),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_x, u_y, u_t = grads_u[..., 0:1], grads_u[..., 1:2], grads_u[..., 2:3]\n",
    "    v_x, v_y, v_t = grads_v[..., 0:1], grads_v[..., 1:2], grads_v[..., 2:3]\n",
    "    p_x, p_y = grads_p[..., 0:1], grads_p[..., 1:2]\n",
    "\n",
    "    # Second derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, xyt, grad_outputs=torch.ones_like(u_x),\n",
    "                              create_graph=True, retain_graph=True)[0][..., 0:1]\n",
    "    u_yy = torch.autograd.grad(u_y, xyt, grad_outputs=torch.ones_like(u_y),\n",
    "                              create_graph=True, retain_graph=True)[0][..., 1:2]\n",
    "    v_xx = torch.autograd.grad(v_x, xyt, grad_outputs=torch.ones_like(v_x),\n",
    "                              create_graph=True, retain_graph=True)[0][..., 0:1]\n",
    "    v_yy = torch.autograd.grad(v_y, xyt, grad_outputs=torch.ones_like(v_y),\n",
    "                              create_graph=True, retain_graph=True)[0][..., 1:2]\n",
    "\n",
    "    # Momentum equations\n",
    "    r_u = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    r_v = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return r_u, r_v, u, v\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Data Handling\n",
    "# =============================================================================\n",
    "\n",
    "class CylinderWakeData:\n",
    "    \"\"\"Loads and manages cylinder wake dataset.\"\"\"\n",
    "    def __init__(self, mat_path: str, seed: int = 0):\n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"Could not find {mat_path}\")\n",
    "        \n",
    "        data = scipy.io.loadmat(mat_path)\n",
    "        self.U_star = data[\"U_star\"]  # (N,2,T)\n",
    "        self.p_star = data[\"p_star\"]  # (N,T)\n",
    "        self.t_star = data[\"t\"]       # (T,1)\n",
    "        self.X_star = data[\"X_star\"]  # (N,2)\n",
    "\n",
    "        self.N = self.X_star.shape[0]\n",
    "        self.T = self.t_star.shape[0]\n",
    "\n",
    "        # Flatten to NT points\n",
    "        XX = np.tile(self.X_star[:, 0:1], (1, self.T))\n",
    "        YY = np.tile(self.X_star[:, 1:2], (1, self.T))\n",
    "        TT = np.tile(self.t_star, (1, self.N)).T\n",
    "\n",
    "        UU = self.U_star[:, 0, :]\n",
    "        VV = self.U_star[:, 1, :]\n",
    "        PP = self.p_star\n",
    "\n",
    "        self.x = XX.flatten()[:, None]\n",
    "        self.y = YY.flatten()[:, None]\n",
    "        self.t = TT.flatten()[:, None]\n",
    "        self.u = UU.flatten()[:, None]\n",
    "        self.v = VV.flatten()[:, None]\n",
    "        self.p = PP.flatten()[:, None]\n",
    "\n",
    "        self.NT = self.x.shape[0]\n",
    "        \n",
    "        # For fixed training set (like PINN)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "    def get_fixed_training_set(self, n: int):\n",
    "        \"\"\"Get fixed training set like PINN (sampled once).\"\"\"\n",
    "        idx = self.rng.choice(self.NT, n, replace=False)\n",
    "        xyt = np.concatenate([self.x[idx], self.y[idx], self.t[idx]], axis=1)\n",
    "        uvp = np.concatenate([self.u[idx], self.v[idx], self.p[idx]], axis=1)\n",
    "        return xyt, uvp\n",
    "\n",
    "    def sample(self, n: int):\n",
    "        \"\"\"Random sampling (for dynamic collocation).\"\"\"\n",
    "        idx = np.random.choice(self.NT, n, replace=False)\n",
    "        xyt = np.concatenate([self.x[idx], self.y[idx], self.t[idx]], axis=1)\n",
    "        uvp = np.concatenate([self.u[idx], self.v[idx], self.p[idx]], axis=1)\n",
    "        return xyt, uvp\n",
    "\n",
    "    def snapshot(self, t_index: int):\n",
    "        \"\"\"Get full spatial snapshot at specific time index.\"\"\"\n",
    "        t_index = int(np.clip(t_index, 0, self.T - 1))\n",
    "        x = self.X_star[:, 0:1]\n",
    "        y = self.X_star[:, 1:2]\n",
    "        t = np.full_like(x, self.t_star[t_index, 0])\n",
    "        u = self.U_star[:, 0, t_index:t_index+1]\n",
    "        v = self.U_star[:, 1, t_index:t_index+1]\n",
    "        p = self.p_star[:, t_index:t_index+1]\n",
    "        xyt = np.concatenate([x, y, t], axis=1)\n",
    "        uvp = np.concatenate([u, v, p], axis=1)\n",
    "        return xyt, uvp\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_full_snapshot(model, data: CylinderWakeData, device, t_index: int, \n",
    "                       nu: float, batch_query: int = 8192, mode: str = 'direct'):\n",
    "    \"\"\"Evaluate on full spatial snapshot at given time index.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get full snapshot\n",
    "    xyt_all, uvp_all = data.snapshot(t_index)\n",
    "    N = xyt_all.shape[0]\n",
    "    \n",
    "    # Use same data as context (simulating perfect sparse observations)\n",
    "    # For fair comparison with PINN, we provide all training data as context\n",
    "    ctx_pos = to_device(xyt_all, device).unsqueeze(0)  # (1,N,3)\n",
    "    ctx_feats = to_device(uvp_all[:, 0:2], device).unsqueeze(0)  # (1,N,2)\n",
    "    \n",
    "    # Query all points\n",
    "    xyt_q = to_device(xyt_all, device).unsqueeze(0)\n",
    "    \n",
    "    # Predict in batches\n",
    "    outs = []\n",
    "    for i in range(0, xyt_q.shape[1], batch_query):\n",
    "        out_i = model(ctx_feats, ctx_pos, xyt_q[:, i:i+batch_query, :])\n",
    "        outs.append(out_i)\n",
    "    pred = torch.cat(outs, dim=1).squeeze(0)  # (N, 2 or 3)\n",
    "    \n",
    "    # Extract u, v, p depending on mode\n",
    "    if mode == 'streamfunction':\n",
    "        # Compute u, v from streamfunction (need gradients)\n",
    "        with torch.enable_grad():\n",
    "            xyt_rg = xyt_q.clone().requires_grad_(True)\n",
    "            psi_p_out = model(ctx_feats, ctx_pos, xyt_rg)\n",
    "            psi = psi_p_out[..., 0:1]\n",
    "            \n",
    "            grads_psi = torch.autograd.grad(psi, xyt_rg, \n",
    "                                           grad_outputs=torch.ones_like(psi),\n",
    "                                           create_graph=True)[0]\n",
    "            u_pred = grads_psi[..., 1:2].squeeze(0)  # ∂psi/∂y\n",
    "            v_pred = -grads_psi[..., 0:1].squeeze(0)  # -∂psi/∂x\n",
    "        p_pred = pred[:, 1:2]\n",
    "        uvp_pred = torch.cat([u_pred, v_pred, p_pred], dim=1)\n",
    "    else:\n",
    "        uvp_pred = pred  # Already (u,v,p)\n",
    "    \n",
    "    uvp_true = to_device(uvp_all, device)\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse_u = F.mse_loss(uvp_pred[:, 0], uvp_true[:, 0]).item()\n",
    "    mse_v = F.mse_loss(uvp_pred[:, 1], uvp_true[:, 1]).item()\n",
    "    mse_p = F.mse_loss(uvp_pred[:, 2], uvp_true[:, 2]).item()\n",
    "    mse_total = (mse_u + mse_v + mse_p) / 3.0\n",
    "    \n",
    "    # Compute PDE residuals (need gradients enabled)\n",
    "    with torch.enable_grad():\n",
    "        xyt_rg = xyt_q.clone().requires_grad_(True)\n",
    "        \n",
    "        if mode == 'streamfunction':\n",
    "            psi_p = model(ctx_feats, ctx_pos, xyt_rg)\n",
    "            r_u, r_v, _, _ = ns_residuals_streamfunction(psi_p, xyt_rg, nu)\n",
    "            pde = (r_u.square().mean() + r_v.square().mean()).item()\n",
    "        else:\n",
    "            uvp_q = model(ctx_feats, ctx_pos, xyt_rg)\n",
    "            r_u, r_v, r_c = ns_residuals_direct(uvp_q, xyt_rg, nu)\n",
    "            pde = (r_u.square().mean() + r_v.square().mean() + r_c.square().mean()).item()\n",
    "    \n",
    "    return {\n",
    "        \"mse_u\": mse_u,\n",
    "        \"mse_v\": mse_v,\n",
    "        \"mse_p\": mse_p,\n",
    "        \"mse_total\": mse_total,\n",
    "        \"pde\": pde,\n",
    "        \"uvp_pred\": uvp_pred.detach().cpu().numpy(),\n",
    "        \"uvp_true\": uvp_all,\n",
    "        \"x\": xyt_all[:, 0],\n",
    "        \"y\": xyt_all[:, 1],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20233952-c808-437c-8c44-f55297a2171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "class Config:\n",
    "    # Data\n",
    "    data_path = \"./cylinder_nektar_wake.mat\"\n",
    "    device = \"cuda\"\n",
    "    seed = 0\n",
    "    \n",
    "    # Physics\n",
    "    Re = 100.0\n",
    "    mode = \"physics\"  # 'pure', 'physics', or 'streamfunction'\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Training data\n",
    "    n_train = 2500\n",
    "    n_colloc = 5000\n",
    "    eval_t_index = 100\n",
    "    \n",
    "    # Loss weights\n",
    "    w_data = 1.0\n",
    "    w_p = 1.0\n",
    "    w_pde = 1.0\n",
    "    w_cont = 1.0\n",
    "    \n",
    "    # Model\n",
    "    n_layer = 6\n",
    "    n_head = 8\n",
    "    n_embd = 256\n",
    "    dropout = 0.1\n",
    "    use_checkpointing = False\n",
    "    \n",
    "    # Training\n",
    "    steps = 5000\n",
    "    batch_size = 1\n",
    "    lr = 3e-4\n",
    "    weight_decay = 1e-4\n",
    "    warmup = 500\n",
    "    grad_clip = 1.0\n",
    "    \n",
    "    # Logging\n",
    "    print_every = 500\n",
    "    eval_every = 500\n",
    "    save_every = 2000\n",
    "    save_path = \"checkpoints/pat_physics.pt\"\n",
    "    out_dir = \"outputs/pat_physics\"\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b54ac1-cfdd-43ea-ac4b-c226ef9c3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    device = torch.device(args.device)\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    data = CylinderWakeData(args.data_path, seed=args.seed)\n",
    "    nu = 1.0 / args.Re\n",
    "    \n",
    "    # Get fixed training set (like PINN)\n",
    "    xyt_train, uvp_train = data.get_fixed_training_set(args.n_train)\n",
    "    xyt_train = to_device(xyt_train, device).requires_grad_(True)\n",
    "    uvp_train = to_device(uvp_train, device)\n",
    "    \n",
    "    # PAT config\n",
    "    cfg = PATConfig()\n",
    "    cfg.d_patch = 2  # (u,v) as features\n",
    "    cfg.d_pos = 3    # (x,y,t)\n",
    "    cfg.n_embd = args.n_embd\n",
    "    cfg.n_head = args.n_head\n",
    "    cfg.n_layer = args.n_layer\n",
    "    cfg.dropout = args.dropout\n",
    "    cfg.use_gradient_checkpointing = args.use_checkpointing\n",
    "    cfg.nu_bar = nu\n",
    "    \n",
    "    # Set alpha based on mode\n",
    "    if args.mode == 'pure':\n",
    "        cfg.alpha = 0.0\n",
    "    elif args.mode == 'physics':\n",
    "        cfg.alpha = args.alpha\n",
    "    else:  # streamfunction\n",
    "        cfg.alpha = args.alpha\n",
    "    \n",
    "    model = PATModelNS(cfg, out_dim=3, mode=args.mode).to(device)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PAT for 2D Navier-Stokes - Improved for PINN Comparison\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Mode: {args.mode}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Data: {args.data_path} | N={data.N} | T={data.T}\")\n",
    "    print(f\"Re={args.Re} -> nu={nu:.6f}\")\n",
    "    print(f\"Training points: {args.n_train} (fixed like PINN)\")\n",
    "    print(f\"Collocation points: {args.n_colloc}\")\n",
    "    print(f\"Alpha (physics bias): {cfg.alpha}\")\n",
    "    print(f\"Loss weights: w_data={args.w_data}, w_p={args.w_p}, \" \n",
    "          f\"w_pde={args.w_pde}, w_cont={args.w_cont}\")\n",
    "    print(f\"Steps: {args.steps}, LR: {args.lr}, Batch: {args.batch_size}\")\n",
    "    print(f\"Model parameters: {n_params:,}\")\n",
    "    print(f\"Eval at t_index={args.eval_t_index}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=args.lr, \n",
    "                             weight_decay=args.weight_decay)\n",
    "    \n",
    "    # LR scheduler with warmup + cosine decay\n",
    "    def lr_lambda(step):\n",
    "        if step < args.warmup:\n",
    "            return step / max(args.warmup, 1)\n",
    "        prog = (step - args.warmup) / max(args.steps - args.warmup, 1)\n",
    "        return 0.5 * (1 + math.cos(math.pi * prog))\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"step\": [], \"loss\": [], \"data_uv\": [], \"data_p\": [], \n",
    "        \"pde\": [], \"cont\": [], \"lr\": [],\n",
    "        \"eval_step\": [], \"eval_mse\": [], \"eval_pde\": []\n",
    "    }\n",
    "    \n",
    "    best_eval = float(\"inf\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for step in range(1, args.steps + 1):\n",
    "        model.train()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Build context from training data (can subsample if needed)\n",
    "        if args.batch_size == 1:\n",
    "            # Use all training data as context\n",
    "            ctx_pos = xyt_train.unsqueeze(0)  # (1,N,3)\n",
    "            ctx_feats = uvp_train[:, 0:2].unsqueeze(0)  # (1,N,2)\n",
    "            obs_uvp = uvp_train.unsqueeze(0)  # (1,N,3)\n",
    "        else:\n",
    "            # For batch_size > 1, could subsample differently per batch element\n",
    "            # For now, keep it simple\n",
    "            ctx_pos = xyt_train.unsqueeze(0).expand(args.batch_size, -1, -1)\n",
    "            ctx_feats = uvp_train[:, 0:2].unsqueeze(0).expand(args.batch_size, -1, -1)\n",
    "            obs_uvp = uvp_train.unsqueeze(0).expand(args.batch_size, -1, -1)\n",
    "        \n",
    "        # Sample collocation points\n",
    "        xyt_col_list = []\n",
    "        for _ in range(args.batch_size):\n",
    "            xyt_c, _ = data.sample(args.n_colloc)\n",
    "            xyt_col_list.append(to_device(xyt_c, device))\n",
    "        xyt_col = torch.stack(xyt_col_list, dim=0).requires_grad_(True)\n",
    "        \n",
    "        # Data loss on training points\n",
    "        if args.mode == 'streamfunction':\n",
    "            # For streamfunction, we can't directly supervise u,v\n",
    "            # Instead, supervise on pressure and use PDE loss for velocity\n",
    "            pred = model(ctx_feats, ctx_pos, ctx_pos)\n",
    "            loss_p = F.mse_loss(pred[..., 1:2], obs_uvp[..., 2:3])\n",
    "            loss_data_uv = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            pred = model(ctx_feats, ctx_pos, ctx_pos)\n",
    "            loss_u = F.mse_loss(pred[..., 0:1], obs_uvp[..., 0:1])\n",
    "            loss_v = F.mse_loss(pred[..., 1:2], obs_uvp[..., 1:2])\n",
    "            loss_p = F.mse_loss(pred[..., 2:3], obs_uvp[..., 2:3])\n",
    "            loss_data_uv = loss_u + loss_v\n",
    "        \n",
    "        # PDE residuals at collocation points\n",
    "        pred_col = model(ctx_feats, ctx_pos, xyt_col)\n",
    "        \n",
    "        if args.mode == 'streamfunction':\n",
    "            r_u, r_v, _, _ = ns_residuals_streamfunction(pred_col, xyt_col, nu)\n",
    "            loss_pde = r_u.square().mean() + r_v.square().mean()\n",
    "            loss_cont = torch.tensor(0.0, device=device)  # Continuity automatic\n",
    "        else:\n",
    "            r_u, r_v, r_c = ns_residuals_direct(pred_col, xyt_col, nu)\n",
    "            loss_pde = r_u.square().mean() + r_v.square().mean()\n",
    "            loss_cont = r_c.square().mean()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = (args.w_data * loss_data_uv + \n",
    "                args.w_p * loss_p + \n",
    "                args.w_pde * loss_pde + \n",
    "                args.w_cont * loss_cont)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        optim.step()\n",
    "        sched.step()\n",
    "        \n",
    "        # Logging\n",
    "        if step % args.print_every == 0 or step == 1:\n",
    "            lr_now = sched.get_last_lr()[0]\n",
    "            print(f\"[{step:06d}] loss={loss.item():.3e} | \" \n",
    "                  f\"data(u,v)={loss_data_uv.item():.3e} | \"\n",
    "                  f\"p={loss_p.item():.3e} | pde={loss_pde.item():.3e} | \"\n",
    "                  f\"cont={loss_cont.item():.3e} | lr={lr_now:.2e}\")\n",
    "            \n",
    "            history[\"step\"].append(step)\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            history[\"data_uv\"].append(loss_data_uv.item())\n",
    "            history[\"data_p\"].append(loss_p.item())\n",
    "            history[\"pde\"].append(loss_pde.item())\n",
    "            history[\"cont\"].append(loss_cont.item())\n",
    "            history[\"lr\"].append(lr_now)\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % args.eval_every == 0 or step == args.steps:\n",
    "            res = eval_full_snapshot(model, data, device, args.eval_t_index, \n",
    "                                    nu, mode=args.mode)\n",
    "            print(f\"         EVAL t={args.eval_t_index} | \"\n",
    "                  f\"MSE={res['mse_total']:.3e} \"\n",
    "                  f\"(u={res['mse_u']:.3e}, v={res['mse_v']:.3e}, \"\n",
    "                  f\"p={res['mse_p']:.3e}) | PDE={res['pde']:.3e}\")\n",
    "            \n",
    "            history[\"eval_step\"].append(step)\n",
    "            history[\"eval_mse\"].append(res[\"mse_total\"])\n",
    "            history[\"eval_pde\"].append(res[\"pde\"])\n",
    "            \n",
    "            if res[\"mse_total\"] < best_eval:\n",
    "                best_eval = res[\"mse_total\"]\n",
    "                if args.save_path:\n",
    "                    ensure_dir(args.save_path)\n",
    "                    torch.save({\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"cfg\": cfg.__dict__,\n",
    "                        \"step\": step,\n",
    "                        \"eval_mse\": best_eval,\n",
    "                        \"args\": vars(args),\n",
    "                    }, args.save_path.replace(\".pt\", \"_best.pt\"))\n",
    "                    print(f\"         ✓ Saved best checkpoint\")\n",
    "        \n",
    "        # Periodic checkpoints\n",
    "        if args.save_path and step % args.save_every == 0:\n",
    "            ensure_dir(args.save_path)\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"cfg\": cfg.__dict__,\n",
    "                \"step\": step,\n",
    "                \"history\": history,\n",
    "                \"args\": vars(args),\n",
    "            }, args.save_path)\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nDone! Best MSE={best_eval:.3e}, Time={elapsed/60:.1f} min\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    if args.save_path:\n",
    "        ensure_dir(args.save_path)\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"cfg\": cfg.__dict__,\n",
    "            \"step\": args.steps,\n",
    "            \"history\": history,\n",
    "            \"args\": vars(args),\n",
    "        }, args.save_path.replace(\".pt\", \"_final.pt\"))\n",
    "        print(f\"Saved final checkpoint\")\n",
    "    \n",
    "    # Final evaluation and plot\n",
    "    res = eval_full_snapshot(model, data, device, args.eval_t_index, \n",
    "                            nu, mode=args.mode)\n",
    "    \n",
    "    plot_results(res, history, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ee1480-4879-4d7d-9fda-1342ae007ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(res, history, args):\n",
    "    \"\"\"Create comprehensive result plots.\"\"\"\n",
    "    # 1. Training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.semilogy(history[\"step\"], history[\"loss\"])\n",
    "    ax.set_title(\"Total Loss\")\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.semilogy(history[\"step\"], history[\"data_uv\"], label=\"data(u,v)\")\n",
    "    ax.semilogy(history[\"step\"], history[\"data_p\"], label=\"data(p)\")\n",
    "    ax.semilogy(history[\"step\"], history[\"pde\"], label=\"PDE\")\n",
    "    ax.semilogy(history[\"step\"], history[\"cont\"], label=\"continuity\")\n",
    "    ax.set_title(\"Loss Components\")\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    if len(history[\"eval_step\"]) > 0:\n",
    "        ax.semilogy(history[\"eval_step\"], history[\"eval_mse\"], label=\"MSE\")\n",
    "        ax.semilogy(history[\"eval_step\"], history[\"eval_pde\"], label=\"PDE\")\n",
    "        ax.set_title(\"Evaluation Metrics\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(history[\"step\"], history[\"lr\"])\n",
    "    ax.set_title(\"Learning Rate\")\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(args.out_dir, f\"training_curves_{args.mode}.png\")\n",
    "    ensure_dir(save_path)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved training curves to {save_path}\")\n",
    "    \n",
    "    # 2. Prediction visualization\n",
    "    x = res[\"x\"]\n",
    "    y = res[\"y\"]\n",
    "    uvp_true = res[\"uvp_true\"]\n",
    "    uvp_pred = res[\"uvp_pred\"]\n",
    "    \n",
    "    # Detect grid structure\n",
    "    xu = np.unique(np.round(x, 10))\n",
    "    yu = np.unique(np.round(y, 10))\n",
    "    nx, ny = len(xu), len(yu)\n",
    "    \n",
    "    # Try to reshape to grid\n",
    "    try:\n",
    "        # Create mapping for structured grid\n",
    "        x_to_i = {val: i for i, val in enumerate(xu)}\n",
    "        y_to_j = {val: j for j, val in enumerate(yu)}\n",
    "        \n",
    "        def to_grid(values):\n",
    "            grid = np.full((ny, nx), np.nan)\n",
    "            for xi, yi, val in zip(np.round(x, 10), np.round(y, 10), values):\n",
    "                grid[y_to_j[yi], x_to_i[xi]] = val\n",
    "            return grid\n",
    "        \n",
    "        is_grid = True\n",
    "    except:\n",
    "        is_grid = False\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "    \n",
    "    for i, var in enumerate(['u', 'v', 'p']):\n",
    "        true_val = uvp_true[:, i]\n",
    "        pred_val = uvp_pred[:, i]\n",
    "        err_val = np.abs(pred_val - true_val)\n",
    "        \n",
    "        if is_grid:\n",
    "            # Smooth contour plots\n",
    "            true_grid = to_grid(true_val)\n",
    "            pred_grid = to_grid(pred_val)\n",
    "            err_grid = to_grid(err_val)\n",
    "            \n",
    "            extent = [xu.min(), xu.max(), yu.min(), yu.max()]\n",
    "            \n",
    "            # True\n",
    "            ax = axes[i, 0]\n",
    "            im = ax.imshow(true_grid, origin='lower', aspect='auto', \n",
    "                          extent=extent, cmap='viridis', interpolation='bilinear')\n",
    "            ax.set_title(f'{var} (True)')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "            # Predicted\n",
    "            ax = axes[i, 1]\n",
    "            im = ax.imshow(pred_grid, origin='lower', aspect='auto',\n",
    "                          extent=extent, cmap='viridis', interpolation='bilinear')\n",
    "            ax.set_title(f'{var} (Pred)')\n",
    "            ax.set_xlabel('x')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "            # Error\n",
    "            ax = axes[i, 2]\n",
    "            im = ax.imshow(err_grid, origin='lower', aspect='auto',\n",
    "                          extent=extent, cmap='hot', interpolation='bilinear')\n",
    "            ax.set_title(f'{var} (Abs Error)')\n",
    "            ax.set_xlabel('x')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "        else:\n",
    "            # Fallback to scatter for unstructured data\n",
    "            ax = axes[i, 0]\n",
    "            sc = ax.scatter(x, y, c=true_val, s=3, cmap='viridis')\n",
    "            ax.set_title(f'{var} (True)')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            plt.colorbar(sc, ax=ax)\n",
    "            \n",
    "            ax = axes[i, 1]\n",
    "            sc = ax.scatter(x, y, c=pred_val, s=3, cmap='viridis')\n",
    "            ax.set_title(f'{var} (Pred)')\n",
    "            ax.set_xlabel('x')\n",
    "            plt.colorbar(sc, ax=ax)\n",
    "            \n",
    "            ax = axes[i, 2]\n",
    "            sc = ax.scatter(x, y, c=err_val, s=3, cmap='hot')\n",
    "            ax.set_title(f'{var} (Abs Error)')\n",
    "            ax.set_xlabel('x')\n",
    "            plt.colorbar(sc, ax=ax)\n",
    "    \n",
    "    plt.suptitle(f\"PAT-{args.mode} | MSE={res['mse_total']:.3e} | \"\n",
    "                f\"PDE={res['pde']:.3e} | t_index={args.eval_t_index}\", \n",
    "                y=0.995)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(args.out_dir, f\"predictions_{args.mode}.png\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved predictions to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ecb3548-2378-4e22-99aa-2bf8542ab4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PAT for 2D Navier-Stokes - Improved for PINN Comparison\n",
      "================================================================================\n",
      "Mode: physics\n",
      "Device: cuda\n",
      "Data: ./cylinder_nektar_wake.mat | N=5000 | T=200\n",
      "Re=100.0 -> nu=0.010000\n",
      "Training points: 2500 (fixed like PINN)\n",
      "Collocation points: 5000\n",
      "Alpha (physics bias): 1.0\n",
      "Loss weights: w_data=1.0, w_p=1.0, w_pde=1.0, w_cont=1.0\n",
      "Steps: 5000, LR: 0.0003, Batch: 1\n",
      "Model parameters: 5,630,851\n",
      "Eval at t_index=100\n",
      "================================================================================\n",
      "\n",
      "[000001] loss=8.812e-01 | data(u,v)=8.695e-01 | p=1.165e-02 | pde=1.099e-12 | cont=1.902e-13 | lr=6.00e-07\n",
      "[000500] loss=1.106e-02 | data(u,v)=7.096e-03 | p=9.889e-04 | pde=1.866e-03 | cont=1.105e-03 | lr=3.00e-04\n",
      "         EVAL t=100 | MSE=3.831e-03 (u=5.817e-03, v=4.605e-03, p=1.073e-03) | PDE=3.760e-03\n",
      "         ✓ Saved best checkpoint\n",
      "[001000] loss=2.170e-03 | data(u,v)=9.238e-04 | p=8.682e-05 | pde=7.169e-04 | cont=4.422e-04 | lr=2.91e-04\n",
      "         EVAL t=100 | MSE=7.829e-04 (u=1.198e-03, v=9.017e-04, p=2.490e-04) | PDE=2.327e-03\n",
      "         ✓ Saved best checkpoint\n",
      "[001500] loss=1.893e-03 | data(u,v)=1.406e-03 | p=7.352e-05 | pde=2.927e-04 | cont=1.203e-04 | lr=2.65e-04\n",
      "         EVAL t=100 | MSE=6.915e-04 (u=1.126e-03, v=7.135e-04, p=2.347e-04) | PDE=1.080e-03\n",
      "         ✓ Saved best checkpoint\n",
      "[002000] loss=1.124e-03 | data(u,v)=6.286e-04 | p=3.327e-05 | pde=3.431e-04 | cont=1.187e-04 | lr=2.25e-04\n",
      "         EVAL t=100 | MSE=2.822e-04 (u=3.949e-04, v=2.581e-04, p=1.937e-04) | PDE=1.383e-03\n",
      "         ✓ Saved best checkpoint\n",
      "[002500] loss=9.473e-04 | data(u,v)=5.847e-04 | p=5.836e-05 | pde=1.907e-04 | cont=1.136e-04 | lr=1.76e-04\n",
      "         EVAL t=100 | MSE=4.076e-04 (u=7.244e-04, v=2.552e-04, p=2.432e-04) | PDE=1.267e-03\n",
      "[003000] loss=1.760e-04 | data(u,v)=6.617e-05 | p=5.562e-06 | pde=6.826e-05 | cont=3.597e-05 | lr=1.24e-04\n",
      "         EVAL t=100 | MSE=2.118e-04 (u=2.641e-04, v=1.770e-04, p=1.941e-04) | PDE=1.262e-03\n",
      "         ✓ Saved best checkpoint\n",
      "[003500] loss=9.139e-05 | data(u,v)=2.642e-05 | p=3.295e-06 | pde=4.127e-05 | cont=2.040e-05 | lr=7.50e-05\n",
      "         EVAL t=100 | MSE=2.144e-04 (u=2.836e-04, v=1.557e-04, p=2.041e-04) | PDE=1.306e-03\n",
      "[004000] loss=7.238e-05 | data(u,v)=1.868e-05 | p=3.215e-06 | pde=3.368e-05 | cont=1.680e-05 | lr=3.51e-05\n",
      "         EVAL t=100 | MSE=2.154e-04 (u=2.786e-04, v=1.734e-04, p=1.942e-04) | PDE=1.361e-03\n",
      "[004500] loss=6.353e-05 | data(u,v)=1.511e-05 | p=2.538e-06 | pde=3.034e-05 | cont=1.554e-05 | lr=9.05e-06\n",
      "         EVAL t=100 | MSE=2.151e-04 (u=2.847e-04, v=1.662e-04, p=1.945e-04) | PDE=1.400e-03\n",
      "[005000] loss=5.995e-05 | data(u,v)=1.540e-05 | p=2.539e-06 | pde=2.892e-05 | cont=1.309e-05 | lr=0.00e+00\n",
      "         EVAL t=100 | MSE=2.175e-04 (u=2.878e-04, v=1.677e-04, p=1.969e-04) | PDE=1.428e-03\n",
      "\n",
      "Done! Best MSE=2.118e-04, Time=54.8 min\n",
      "Saved final checkpoint\n",
      "Saved training curves to outputs/pat_physics/training_curves_physics.png\n",
      "Saved predictions to outputs/pat_physics/predictions_physics.png\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "ensure_dir(args.save_path)\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "# Train model\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
